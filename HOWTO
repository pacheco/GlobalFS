** QUICK START
**********************************

- Go into ~/usr/sinergiafs
- run:
    fab -f fabfile-local.py start_all:2

When the script finishes, the system should be running locally with 2 partitions deployed.
That will be:
     2 global acceptors
     3 replicas for each partition (6 total)
     2 fuse clients, mounted at /tmp/fs1 and /tmp/fs2
     2 python HTTP storage servers (one for each partition)

The idea here is that both client mounts (fs1 and fs2) are mounting the same filesystem tree.
The FS is partitioned by path:

   /1* --> stored only on partition one
   /2* --> stored only on partition two

Anything else is replicated on all partitions. For example:

   /1/asdf --> partition 1
   /foo/1  --> replicated
   /2asd   --> partition 2
   /       --> replicated
   /bar1   --> replicated
   /2a/1/a --> partition 2

For a write, the client writes the data to one (if it is a
'local' command) or all (if it is replicated) storages and then to the
respective metadata partitions.

For a read, the client contacts a single replica.

- To kill everything:
    fab -f fabfile-local.py kill_and_clear

** EXTENDING/INTEGRATING
***********************************

When mounting the filesystem, it is possible to select a Storage
implementation. A basic client for an http storage is provided as the
HttpStorage class. It is based on an older version of Raluca's DHT. If
nothing changed, it should just be a matter of adding the address:port
of the servers to a config file in the correct format. Check the
HttpStorage class and the config file
(src/main/resources/storagecfg/3-httpstorage.cfg) for details.

To support a new storage type the Storage interface has to be
implemented.  It has get/put/delete and an initialize method which
receives a config file. The first line in this config file has to be
the full path to the implementation, for example
ch.usi.paxosfs.storage.HttpStorage. The rest of the file is
interpreted by each Storage implementation. The clients (fuse mounts)
receive this config file as a parameter and then instantiate the
correct implementation. Storage calls are made parallel by means of
Future's (java.util.concurrent.Future).

Be sure to have the implementing class available on the classpath - by
copying it to the sinergiafs deploy folder, for example.

For a complete example, check the HttpStorage code and JavaDoc and the
respective unit test (StorageTest.testHttpStorage).

The 'dht-fake.py' script starts a small HTTP server that interfaces
with the HttpStorage.

** COMPILING & DEPLOYING
***************************************
Install the following packets (considering Ubuntu 14.04):

Oracle Java 7
   sudo add-apt-repository ppa:webupd8team/java
   sudo apt-get update
   sudo apt-get install oracle-java7-installer
zookeeper
zookeeperd
dtach
fuse 
libfuse2 
libfuse-dev
maven
fabric
python-kazoo
python-flask
dstat

The zookeeper executables (zkCli.sh specifically) should be available on PATH. They
are generally inside '/usr/share/zookeeper/bin'.

Besides zookeeper, 3 projects are needed:

- URingPaxos - commit 59d1c0574938defa5d22e50310f4c6f660607094 from https://github.com/sambenz/URingPaxos
- fuse4j - commit 729b3bb4c62b66650d97fe7f71eb21d568102a34 from https://github.com/dtrott/fuse4j
- sinergiafs - from https://bitbucket.org/pacheco/sinergiafs

For URingPaxos:
  - mvn clean install -Dmaven.test.skip
  - unzip the target/Paxos-trunk.zip archive into ~/usr/Paxos-trunk

For fuse4j:
  - mvn clean install
  - compile the code (make) inside 'native' and copy libjavafs.so into ~/usr/lib
    - for this it is necessary to use the correct make.flags and set the paths appropriately

For sinergiafs:
  - mvn clean install
  - copy the 'target/sinergia-0.0.1-SNAPSHOT-deploy folder into ~/usr/sinergiafs
    - You can create a softlink there instead as I did in the VM
  - whenever there is need to update from the repository, run:
    - git fetch origin; git rebase; mvn clean install
  

More simply, just copy the ~/workspace and ~/usr folders from the example VM

** DISTRIBUTED DEPLOYMENT
***************************************

The following components have to be deployed/started:

- zookeeper
- 2-3 URingPaxos acceptors for the global ring
- 2-3 FS replicas for each partition
- 1 storage deployment (DHT) for each partition
- client mount points

Checking the fabfile-local.py script (starting from the 'start_all'
function) is probably the best way to figure out what is required to
deploy each part and in which order it should be done.

Some random points:

  - I use the 'dtach' command to keep the processes running in the background.

  - After starting all the servers, it takes some time for paxos to
    'stabilize'. There is a small java program to check when the
    system is ready (exits with 0 when OK, 1 if not). Check the
    'paxos_on' function inside fabfile-local.py.

  ** DISCLAIMER** - it might happen that paxos doesn't boot up
     correctly for some reason (not too often). When this happens,
     this program will keep returning 1. If that is the case, kill
     everything and restart the system.

  - I've been using fabric (a python library) to script the deployment
    on EC2. It has been useful for me but you can use whatever is
    easier.
